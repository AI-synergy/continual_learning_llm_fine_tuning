# README

Welcome to the **Continual-Learning LLM Fine-Tuning Pipeline** repository. This project demonstrates an efficient workflow for sequentially fine-tuning a language model on multiple speaker styles while minimizing catastrophic forgetting.

## Repository Structure

```
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ home_task.ipynb      # Main Jupyter notebook illustrating the pipeline
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ lee_cronin3.csv      # Lee Cronin with Lex Fridman dialogue dataset
â”‚   â””â”€â”€ lisa_randall.csv     # Lisa Randall with Lex Fridman dialogue dataset
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ phase1/              # Saved model and tokenizer after Phase 1 fine-tuning
â”‚   â””â”€â”€ phase2/              # Saved model and tokenizer after Phase 2 continual learning
â”œâ”€â”€ README.md                # Project overview and instructions
â””â”€â”€ requirements.txt         # Required Python packages
```

## Setup and Installation

1. **Clone the repository**:

   ```bash
   git clone https://github.com/AI-synergy/cont-lrn-llm-finetune.git
   cd cont-lrn-llm-finetune
   ```
2. **Install dependencies** (recommended in a Python 3 virtual environment):

   ```bash
   pip install -r requirements.txt
   ```

## Usage

1. **Place the data files** `lee_cronin3.csv` and `lisa_randall.csv` into the `data/` folder.
2. **Launch Jupyter** and open `notebooks/home_task.ipynb`:

   ```bash
   jupyter lab
   ```
3. **Run all cells** in sequence. The notebook covers:

   * Data exploration and EDA
   * Phase 1: LoRA adapter fine-tuning on Lee Cronin
   * Phase 2: Continual learning with EWC + replay on Lisa Randall
   * Forgetting evaluation and ablation studies
4. **Inspect the results** saved under `checkpoints/phase1` and `checkpoints/phase2`.

## Further Reading

For more details, see the report below or refer to the notebookâ€™s Markdown annotations.

---

**Project Title:** Continual-Learning LLM Fine-Tuning Pipeline Continual-Learning LLM Fine-Tuning Pipeline
**Description:** Developed a CPU-efficient workflow leveraging LoRA adapters and Elastic Weight Consolidation to sequentially fine-tune a language model on multiple speaker styles with under 8% forgetting.

**Cold Email Pitch Title:** Continual-Learning LLM Fine-Tuning Showcase
**Cold Email Pitch:** I built a CPU-only, parameter-efficient fine-tuning pipeline using LoRA and EWC to teach an LLM two speaker styles sequentially, achieving under 8% forgettingâ€”happy to share details!

# ğŸš€ Continualâ€‘Learning LLM Fineâ€‘Tuning

**By Krishna Chaitanya**

> â€œCan we teach a small language model two different conversational styles in sequence, and have it remember the first style after learning the second?â€

This solution walks through a complete, CPUâ€‘friendly pipeline that addresses this question using two interview datasets: first Lee Cronin with Lex Fridman, then Lisa Randall with Lex Fridman. We apply parameterâ€‘efficient fineâ€‘tuning followed by continualâ€‘learning techniques to preserve the original conversational style, quantifying forgetting with perplexity measurements.

---

## 1. Motivation & Overview

We set out to build an LLM training workflow that can:

1. **Learn one conversational style** (Lee Cronin â†” Lex Fridman)
2. **Then learn a second style** (Lisa Randall â†” Lex Fridman)

**Goal:** Ensure the model does not lose the ability to generate the first style after sequential training.

**Approach combines:**

* **LoRA adapters** for efficient parameterâ€‘light fineâ€‘tuning.
* **Replay buffer** of previous data mixed with **Elastic Weight Consolidation (EWC)** to protect important weights.
* **Perplexity-based evaluation** on heldâ€‘out original data before and after PhaseÂ 2 to measure forgetting.

These steps create a stable pipeline usable on modest hardware (CPU-only, 16â€¯GB RAM).

---

## 2. Data & Exploratory Analysis

We work with two CSV files, each containing alternating dialogue lines and speaker labels:

* **Dataset 1:** 791 utterances between Lee Cronin and Lex Fridman.
* **Dataset 2:** 219 utterances between Lisa Randall and Lex Fridman.

**Key exploration steps included:**

1. **Speaker normalization:** Standardized speaker names and introduced special tokens (`<LEX>`, `<LEE>`, `<LISA>`) so the model explicitly knows who is speaking.
2. **Utterance length analysis:** Examined character and token length distributions and found over 95â€¯% of lines were under 100 tokens, enabling efficient batching without truncation.
3. **Balance check:** Verified no speaker exceeds 60â€¯% of total turns, avoiding severe class imbalance.
4. **Quality inspection:** Confirmed there were no missing values, reviewed shortest/longest lines for formatting issues, and estimated vocabulary sizes (\~3â€¯035 unique tokens for Lee, \~1â€¯409 for Lisa).

*These insights guided tokenizer settings and hyperparameter choices for subsequent fineâ€‘tuning.*

---

## 3. Model & Tokenizer Configuration

We selected **DistilGPTâ€‘2** (â‰ˆ82â€¯M parameters), a causal language model that runs comfortably on a single CPU. Then we:

* **Extended the tokenizer** to include a dedicated `[PAD]` token and our speaker tokens (`<LEX>`, `<LEE>`, `<LISA>`), ensuring clear speaker signals.
* **Resized model embeddings** to accommodate the new tokens without discarding pretrained weights.
* **Fixed max sequence length** at 100 tokens, covering the majority of dialogue turns without wasted compute.

*This setup provided a solid base for both phases of fineâ€‘tuning.*

---

## 4. PhaseÂ 1: LoRA Adapter Fineâ€‘Tuning on Lee Cronin â†” Lex Fridman

In the first phase, we taught the model to generate both Leeâ€™s and Lexâ€™s lines. Instead of updating all model weights, we injected **LoRA** modules into GPTâ€‘2â€™s attention and projection layers. By training only these small, rankâ€‘reduced matrices, we kept the trainable parameter count under one million, reducing memory use and enabling quick iteration.

**After one epoch on the Lee Cronin data:**

* **Trainable LoRA parameters:** 811â€¯008
* **Validation loss:** 4.8207 â†’ **Perplexity â‰ˆ 124.3**

**After two epochs + linear LR decay (10% warmup):**

* **Validation loss:** 4.5505 â†’ **Perplexity â‰ˆ 94.7**
* **Relative improvement:** \~24% reduction in perplexity (124.3â†’94.7)

**Sample generation:**

```
<LEE> How would you approach cross-coupling for this substrate?
Model â†’ â€œA Suzuki coupling using Pd(PPhâ‚ƒ)â‚„ under inert argon, followed by silica chromatographyâ€¦â€
```

This demonstrates that the adapter captures Lee Croninâ€™s conversational patterns effectively.

---

## 5. PhaseÂ 2: Continual Learning on Lisa Randall â†” Lex Fridman

Sequential fineâ€‘tuning risks **catastrophic forgetting**. To balance **plasticity** (learning Lisaâ€™s style) and **stability** (retaining Leeâ€™s), we combined two methods:

1. **Replay Buffer:** Randomly sampled 10â€¯% of the Lee Cronin data and interleaved it with the Lisa data during PhaseÂ 2 training, rehearsing original knowledge.
2. **Elastic Weight Consolidation (EWC):** Computed the diagonal of the Fisher Information Matrix on replay examples to estimate parameter importance for PhaseÂ 1. Added a penalty term $\lambda \sum_i F_i(\theta_i - \theta^*_i)^2$ to discourage changing critical weights, where $\lambda$ controls penalty strength.

We experimented with different $\lambda$ values and replay proportions, finding $\lambda=10$ with 10â€¯% replay best minimized forgetting.

---

## 6. Measuring Forgetting with Perplexity

We evaluated perplexity on the heldâ€‘out Lee Cronin test set **before** and **after** PhaseÂ 2 under three training configurations:

1. **PhaseÂ 1 = 1 epoch; PhaseÂ 2 = 1 epoch**

   * **Baseline PPL:** â‰ˆ 97.8
   * **After P2 PPL:** â‰ˆ 105.6
   * **Forgetting:** +8.0 %

2. **PhaseÂ 1 = 2 epochs + linear decay; PhaseÂ 2 = 1 epoch**

   * **Baseline PPL:** â‰ˆ 92.7
   * **After P2 PPL:** â‰ˆ 100.24
   * **Forgetting:** +8.1 %

3. **PhaseÂ 1 = 2 epochs + linear decay; PhaseÂ 2 = 2 epochs**

   * **Baseline PPL:** â‰ˆ 90.25
   * **After P2 PPL:** â‰ˆ 96.94
   * **Forgetting:** +7.4 %

> **Interpretation:**
>
> * Adding a second epoch to PhaseÂ 1 alone yields a stronger base (PPL 92.7 â†’ 90.25) but similar forgetting when PhaseÂ 2 is 1 epoch (+8.1 %).
> * Increasing PhaseÂ 2 to 2 epochs further reduces forgetting to **+7.4 %**, demonstrating that additional PhaseÂ 2 training can reclaim some of the original style performance.
> * The final postâ€‘PhaseÂ 2 PPL is lowest in the 2Ã—2 setup (96.94 vs. 100.24 and 105.6), indicating the best overall retention and adaptation tradeâ€‘off.

*These results underscore how tuning both PhaseÂ 1 and PhaseÂ 2 epoch counts can optimize stability vs. plasticity on CPUâ€‘only hardware.*

\--- the complex stabilityâ€“plasticity trade-off: stronger PhaseÂ 1 models require careful PhaseÂ 2 tuning to preserve gains.\*

\---â€¯% rise for the 1â€‘epoch setup demonstrates a strong stabilityâ€“plasticity trade-off given CPU-only constraints.\*

An 8â€¯% rise represents modest forgetting given our single-epoch, CPU-only budget, demonstrating a strong stabilityâ€“plasticity trade-off.

---

## 7. Future Research Directions

To push the boundaries of continual learning and further reduce forgetting, several advanced methods could be explored in future work:

### 7.1 Learning without Forgetting (LwF)

* **Concept:** Use the modelâ€™s own PhaseÂ 1 outputs (soft logits) on Lee Cronin examples as auxiliary targets during PhaseÂ 2, alongside the hard labels for Lisa Randall.
* **Pros:** No need to store actual old data; soft targets carry rich information about original predictions.
* **Cons:** Requires careful balancing between hard and soft losses; noisy soft labels may slow convergence.

### 7.2 Synaptic Intelligence (SI)

* **Concept:** Compute a running estimate of each parameterâ€™s importance during PhaseÂ 1 training (based on weight changes and loss gradients), then penalize changes to important weights during PhaseÂ 2.
* **Pros:** Online computation avoids separate Fisher estimation; lightweight memory footprint.
* **Cons:** Importance estimates can drift if PhaseÂ 1 is very short; hyperparameters (e.g., damping) need tuning.

### 7.3 Gradient Episodic Memory (GEM & AGEM)

* **Concept:** Maintain a small memory of PhaseÂ 1 examples; at each PhaseÂ 2 update, solve a constrained optimization so that the loss on memory does not increase.
* **Pros:** Guarantees no forgetting on stored examples; effective at preserving performance.
* **Cons:** Requires solving a quadratic program or projection per updateâ€”computationally expensive on CPU.

### 7.4 Generative Replay

* **Concept:** Train a compact generative model to reproduce representative Lee Cronin utterances; sample pseudo-experiences from it during PhaseÂ 2 instead of storing raw data.
* **Pros:** No need for raw data storage; the generator can compress information about past tasks.
* **Cons:** Quality of pseudo-data depends on generator fidelity; adds complexity by introducing a second model.

### 7.5 Progressive Networks

* **Concept:** Freeze the PhaseÂ 1 LoRA adapters and base weights, then allocate new adapters for PhaseÂ 2. Lateral connections allow information flow between old and new modules.
* **Pros:** Zero interference with PhaseÂ 1 parameters; strong transfer potential through feature reuse.
* **Cons:** Parameter growth is linear with number of tasks; increases inference cost over time.

### 7.6 Adapter Fusion

* **Concept:** Train separate LoRA adapters for each speaker style and then learn a small fusion network that dynamically weights their contributions at inference time.
* **Pros:** Enables mixing of multiple styles; adapters remain modular and reusable.
* **Cons:** Requires additional trainable fusion parameters; fusion strategy must be learned carefully.

### 7.7 Metaâ€‘Learning for Continual Learning

* **Concept:** Use meta-optimization (e.g., MAML-style) to find initial weights or adapter configurations that are robust to sequential fine-tuning.
* **Pros:** Can yield models that require minimal adaptation steps per new style; potential for one-shot style acquisition.
* **Cons:** Adds a costly meta-training loop; stability of meta-objective on CPU may be challenging.

> Exploring these methods in ablation studies would deepen our understanding of the stabilityâ€“plasticity trade-offs and guide the design of even more robust, CPU-friendly continual-learning pipelines.

---

## 8. Hyperparameter Tuning & Best Practices

Fine-tuning performance and retention can often be improved by thoughtful hyperparameter adjustments. Below are several parameters you might consider increasing or tuning, especially if you have extra time or hardware headroom:

* **Number of Epochs:**

  * **Current:** 1 epoch per phase.
  * **Recommendation:** Increasing to 2â€“3 epochs can allow the model to refine both styles more deeply. Monitor validation loss to avoid overfitting or excessive forgetting.

* **Learning Rate Schedule:**

  * **Current:** Fixed LR of 2Ã—10â»â´.
  * **Recommendation:** Use a **linear decay** or **cosine schedule** with warmup (e.g., 10â€“20% of steps) to stabilize early training and lower the LR gradually toward later epochs.

* **Gradient Accumulation / Batch Size:**

  * **Current:** Batch size 2 with 16Ã— gradient accumulation (effective 32).
  * **Recommendation:** If memory allows, increase effective batch size (e.g., accumulation 32, effective 64) to smooth gradient estimates, improving convergence and reducing noise in EWC penalty.

* **LoRA Rank & Dropout:**

  * **Current:** rank=16, dropout=0.05.
  * **Recommendation:** Test higher rank values (e.g., 32 or 64) to capture more nuanced style features, and tune dropout (0.1â€“0.2) to prevent adapter overfitting, especially on small datasets.

* **EWC Penalty Strength & Replay Ratio:**

  * **Current:** Î»=10, replay=10%.
  * **Recommendation:** For stronger stability, you can increase Î» (up to 20) or replay buffer size (15â€“20%). However, larger EWC penalties may slow learning of the new style, so balance accordingly.

* **Early Stopping & Checkpointing:**

  * Use **early stopping** on validation loss to prevent unnecessary epochs, and **save\_total\_limit** to retain only top-K checkpoints, preserving storage.

> By iteratively tuning these parameters and observing both style fidelity and forgetting metrics, you can further optimize the stabilityâ€“plasticity trade-off. Always track against a held-out validation set for each phase.

---

## 9. How to Run This Notebook How to Run This Notebook

1. **Install** dependencies: `transformers`, `datasets`, `peft`, `torch`, `ipywidgets`, etc.
2. **Place** `lee_cronin3.csv` & `lisa_randall.csv` in the notebook directory.
3. **Open** the notebook in Jupyter and **Run All** cells top-to-bottom.
4. **Checkpoints** will be saved under `checkpoints/phase1` and `checkpoints/phase2`.

---

## ğŸ‰ Conclusion

This comprehensive pipeline demonstrates:

* **Parameterâ€‘efficient adaptation** via LoRA.
* **Continualâ€‘learning strategies** (EWC + replay) to mitigate catastrophic forgetting.
* **Quantitative evaluation** with perplexity comparisons showing only +8â€¯% forgetting while learning a new styleâ€”all on CPU-only hardware.

By implementing and tuning these techniques, we achieve robust LLM performance under real-world constraints. Thank you for reviewingâ€”happy to discuss any questions or next steps!
